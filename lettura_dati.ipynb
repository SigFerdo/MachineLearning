{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-elaborazione dei dati\n",
    "\n",
    "In questa sezione, viene eseguita la pre-elaborazione dei dati per la successiva addestramento della rete neurale. Il processo di pre-elaborazione comprende la lettura dei dati dal file CSV, la normalizzazione delle feature utilizzando la tecnica di MinMaxScaler e la suddivisione del dataset in set di addestramento, convalida e test.\n",
    "\n",
    "- **Lettura dei dati**: I dati vengono letti dal file CSV utilizzando la libreria `pandas`. Il file `diabetes.csv` contiene le informazioni sulle analisi dei pazienti con diabete.\n",
    "\n",
    "- **Normalizzazione delle feature**: Per garantire che tutte le feature abbiano la stessa scala, viene utilizzato il `MinMaxScaler` di scikit-learn per normalizzare le feature nel range [0, 1].\n",
    "\n",
    "- **Suddivisione dei dati**: Il dataset normalizzato viene suddiviso in tre set: addestramento, convalida e test. Questo viene fatto utilizzando la funzione `train_test_split` di scikit-learn. In questo caso, il 70% dei dati viene utilizzato per l'addestramento, il 15% per la convalida e il restante 15% per il test.\n",
    "\n",
    "- **Salvataggio dei dati**: I set di dati risultanti vengono salvati in file separati utilizzando la funzione `np.save` di NumPy. Vengono salvati i seguenti file:\n",
    "    - `train_features.npy`: feature del set di addestramento\n",
    "    - `train_labels.npy`: etichette del set di addestramento\n",
    "    - `val_features.npy`: feature del set di convalida\n",
    "    - `val_labels.npy`: etichette del set di convalida\n",
    "    - `test_features.npy`: feature del set di test\n",
    "    - `test_labels.npy`: etichette del set di test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prima train_features 537\n",
      "prima test_features 231\n",
      "train_features 429\n",
      "test_features 231\n",
      "val_features 108\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Leggi i dati dal file CSV\n",
    "data = pd.read_csv('Dataset/diabetes.csv')\n",
    "\n",
    "# Separate le features di input dalla classe di output\n",
    "features = data.drop(['Outcome'], axis=1)\n",
    "labels = data['Outcome']\n",
    "\n",
    "# Crea un oggetto MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Adatta lo scaler sulle features di input\n",
    "scaler.fit(features)\n",
    "\n",
    "# Trasforma le features di input utilizzando lo scaler\n",
    "features_normalized = scaler.transform(features)\n",
    "\n",
    "# Suddivide i dati in training, validation e test set\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features_normalized, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"prima train_features\",len(train_features))\n",
    "print(\"prima test_features\",len(test_features))\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Calcola la ponderazione delle classi\n",
    "class_labels = np.unique(train_labels)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=train_labels)\n",
    "\n",
    "\n",
    "# Crea un dizionario per memorizzare la ponderazione delle classi\n",
    "class_weights_dict = dict(zip(class_labels, class_weights))\n",
    "\n",
    "# Salva la ponderazione delle classi in un file\n",
    "np.save('Pre_Processed_Data/class_weights.npy', class_weights_dict)\n",
    "\n",
    "\n",
    "# Salva i dati pre-processati nei file\n",
    "np.save('Pre_Processed_Data/train_features.npy', train_features)\n",
    "np.save('Pre_Processed_Data/train_labels.npy', train_labels)\n",
    "np.save('Pre_Processed_Data/val_features.npy', val_features)\n",
    "np.save('Pre_Processed_Data/val_labels.npy', val_labels)\n",
    "np.save('Pre_Processed_Data/test_features.npy', test_features)\n",
    "np.save('Pre_Processed_Data/test_labels.npy', test_labels)\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_features\",len(train_features))\n",
    "print(\"test_features\",len(test_features))\n",
    "print(\"val_features\",len(val_features))\n",
    "\n",
    "\n",
    "print(len(features_normalized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
